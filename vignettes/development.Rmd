---
title: "Area Calculator Development - DRAFT"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Area Calculator Development - DRAFT}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  fig.width = 5
)
```

```{r setup}
suppressWarnings(suppressMessages(library(kraken)))
library(magrittr)
library(dplyr)
library(sf)
library(ggplot2)
library(RColorBrewer)
library(forcats)
```

## Development Summary (DRAFT)

Below is a summary of long-term and short-term development ideas to make running the area calculator model easier. These comments and suggestions are preliminary and need further discussion to verify.


### New warnings and messages needed

- Messages for 'additional', '555' or '999' transects? i.e. Reference or additional sampling>?
- Warnings if no ellipse can be calculated (see below for un-coded failures modes)

### No good reporting output from kraken - makes testing, developing, and debugging hard.

Currently, outputs are held in various lists from different functions. Update kraken() function to produce nice map plot and compliance report?

```{r}
data <- read.csv(system.file("extdat", "test-data/west-of-burwick-iqi-data.csv",
  package = "kraken"
), check.names = FALSE)

result <- kraken(data, loess = TRUE, good_moderate = 0.60)
```


### ❌Need ability to change IQI ratio / EQR

If IQI scores from reference stations are worse (or better?) than IQI model suggests, then need ability to update EQR ratio.

For example 0.60:

```{r message=TRUE, warning=FALSE}
data <- read.csv(system.file("extdat", "test-data/west-of-burwick-iqi-data.csv",
  package = "kraken"
), check.names = FALSE)

result <- kraken(data, loess = FALSE, good_moderate = 0.60)
distances <- result %>% filter(question == "breachDistanceBestFit")
distances$transect <- 1:nrow(distances)
knitr::kable(select(distances, location_id, question, response) %>% mutate(location_id = gsub(pattern = "NA", replacement = "", .$location_id) ))

breach <- result %>%  filter(question == "breachPositionEnsemble")
breach <- breach$object[[1]]

warnings <- result %>%  filter(question == "ellipse_warnings")
warnings$object[[1]]

info <- result %>%  filter(question == "model_info")
info$object[[1]]

result %>%  filter(question == "area_95_confidence")
map <- result %>% filter(question == "map")
map <- map$object[[1]]


```


### ❌Ellipse over estimates area of impact when fitting to stations from six transects?

```{r}

data <- read.csv(system.file("extdat", "test-data/harris-iqi-data.csv",
  package = "kraken"
), check.names = FALSE)


pens <- read.csv(system.file("extdat", "test-data/harris-pen-layout.csv",
  package = "kraken"
), check.names = FALSE)

pens <- filter(pens, Type == "square")
bng <- sf::st_crs(
  "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs"
)

points <- st_as_sf(x = pens, 
                   coords = c("Easting", "Northing"),
                   crs = bng)


points <- sf::st_transform(points,
  crs = "EPSG:4326")

data <- consecutive_stations(data)
probs <- probability_non_linear(data$survey_data, loess = TRUE)
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)

# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
polygon <- areas$polygon
myColors <- c(
  "#d7191c",
  "#fdae61",
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)
blue_theme <- theme(
  panel.background = element_rect(
    fill = "#BFD5E3", colour = "#6D9EC1",
    size = 2, linetype = "solid"
  ),
  panel.grid.major = element_line(
    size = 0.5, linetype = "solid",
    colour = "white"
  ),
  panel.grid.minor = element_line(
    size = 0.25, linetype = "solid",
    colour = "white"
  )
)

names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  geom_sf(data = polygon, alpha = 0, colour = "purple") +
  geom_sf(data = points, colour = "black") +
  colScale +
  blue_theme
g 

```

**Difference**

```{r}

print(paste("Ellipse:", round(st_area(ellipse))))

print(paste("Polygon: ", round(st_area(polygon))))
```

### ❌Pen groups

Currently, we can't automatically deal with separate calculations for pen groups. The above six transect survey would better be calculated as two separate pen groups/ellipses and the two areas merged.

```{r, message=FALSE, warning=FALSE}


transects_pen_1 <- filter(data, Transect %in% c(6,1,2))
transects_pen_2 <- filter(data, Transect %in% c(4,5,3))

pen_1 <- consecutive_stations(transects_pen_1)
probs_1 <- suppressWarnings( probability_non_linear(pen_1$survey_data, loess = TRUE))
breach_1 <- breach(probs_1)
area_1 <- kraken::area(breach_1)

pen_2 <- consecutive_stations(transects_pen_2)
probs_2 <- suppressWarnings(probability_non_linear(pen_2$survey_data, loess = TRUE))
breach_2 <- breach(probs_2)
area_2 <- kraken::area(breach_2)

myColors <- c(
  "#d7191c",
  "#fdae61",
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
transects_pen_1$`WFD status` <- as.factor(transects_pen_1$`WFD status`)
transects_pen_1$`WFD status` <- fct_relevel(
  transects_pen_1$`WFD status`,
  "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)

transects_pen_2$`WFD status` <- as.factor(transects_pen_2$`WFD status`)
transects_pen_2$`WFD status` <- fct_relevel(
  transects_pen_2$`WFD status`,
  "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)
blue_theme <- theme(
  panel.background = element_rect(
    fill = "#BFD5E3", colour = "#6D9EC1",
    size = 2, linetype = "solid"
  ),
  panel.grid.major = element_line(
    size = 0.5, linetype = "solid",
    colour = "white"
  ),
  panel.grid.minor = element_line(
    size = 0.5, linetype = "solid",
    colour = "white"
  )
)

names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = transects_pen_1, aes(color = `WFD status`)) +
   geom_sf(data = transects_pen_2, aes(color = `WFD status`)) +
  geom_sf(data = area_1$ellipse, alpha = 0) +
  geom_sf(data = area_2$ellipse, alpha = 0) +
  geom_sf(data = points, colour = "black") +
  colScale +
  blue_theme
g

single_sf <- dplyr::bind_rows(list(area_2$ellipse,area_1$ellipse))
union <- st_union(single_sf)
sum <- area_2$fifthPercentileArea[[1]][1] + area_1$fifthPercentileArea[[1]][1]


```

Merge both 95% confidence area:
  
Ellipses Area 1: `r round(area_1$fifthPercentileArea[[1]][1])`
  
Ellipses Area 2: `r round(area_2$fifthPercentileArea[[1]][1])`
  
Sum Area: `r round(sum)`
  
Note 95% confidence area is different from merging actual polygon:
  
Union area: `r round(st_area(union))`
  
Issue if polygons overlap - how can we give union/merged area?


### ❌ Area of ellipse displayed can be significantly different to area of 95% percentile reported for compliance

```{r, warning=FALSE}

paste("Area used for compliance:", round(areas$fifthPercentileArea$`5%`))

paste("Size of ellispse used for displayed on map:", round(sf::st_area(areas$ellipse)))

paste(
  "Percentage diff: ",
  100 - round(100 / as.numeric(sf::st_area(areas$ellipse)) * areas$fifthPercentileArea$`5%`), "%"
)
```

### ❌ Two passing samples from each transect can give smaller area than if all samples are analysed.

```{r adjacent, message=FALSE}

# Create mock survey dataframe
data <- demo_iqi
data$IQI <-
  c(
    0.60, 0.61, 0.62, 0.63, 0.64, 0.66, 0.80, 0.80, 0.80,
    0.60, 0.61, 0.62, 0.63, 0.64, 0.66, 0.80,
    0.60, 0.61, 0.62, 0.63, 0.64, 0.66, 0.80,
    0.60, 0.61, 0.62, 0.63, 0.64, 0.66, 0.80
  )



data <- consecutive_stations(data)
knitr::kable(data$sample_point_checks[1, c("twoConsecutiveStations")],
  col.names = "Adjacent"
)

probs <- probability_non_linear(data$survey_data)
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)


# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  # '#fdae61',
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  # "Poor",
  "Moderate",
  "Good",
  "High"
)
blue_theme <- theme(
  panel.background = element_rect(
    fill = "#BFD5E3", colour = "#6D9EC1",
    size = 2, linetype = "solid"
  ),
  panel.grid.major = element_line(
    size = 0.5, linetype = "solid",
    colour = "white"
  ),
  panel.grid.minor = element_line(
    size = 0.25, linetype = "solid",
    colour = "white"
  )
)

names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

Area with 7 stations analysed:

```{r}
areas$fifthPercentileArea$`5%`
```

Area if only two consecutive samples analysed

```{r seven, message=FALSE, warning=FALSE}

# Create mock survey dataframe
data <- demo_iqi
data <- data[c(5:6, 14:15, 21:22, 28:29), ]
data$IQI <- c(
  0.64, 0.66,
  0.64, 0.66,
  0.64, 0.66,
  0.64, 0.66
)


data <- consecutive_stations(data)
knitr::kable(data$sample_point_checks[1, c("twoConsecutiveStations")],
  col.names = "Adjacent"
)

probs <- suppressWarnings(probability_non_linear(data$survey_data))
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)


# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  # '#fdae61',
  # "#fecc5c",
  "#abdda4"
  #  '#2b83ba'
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  # "Poor",
  #  "Moderate",
  "Good"
  # "High"
)
blue_theme <- theme(
  panel.background = element_rect(
    fill = "#BFD5E3", colour = "#6D9EC1",
    size = 2, linetype = "solid"
  ),
  panel.grid.major = element_line(
    size = 0.5, linetype = "solid",
    colour = "white"
  ),
  panel.grid.minor = element_line(
    size = 0.25, linetype = "solid",
    colour = "white"
  )
)

names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

Area with two consecutive station analysed:

```{r}
areas$fifthPercentileArea$`5%`
```

The area calculated from 7 stations is greater than based on 2 stations. This could mean an area passes or fails based on the analysis approach used. Either come is possible; reduced analysis passes and 7 stations fail or vice-versa.

Additionally, a reduced analysis doesn't provide 95% statistical confidence in Failure.

### ❌ Two non-adjacent monitoring stations at Good will fail

Failure is detected but ellipse is still drawn around outer most stations

```{r non_adajcent_table}
# Create mock survey dataframe
data <- demo_iqi
data <- data[c(1:3, 10:12, 17:19, 24:26), ]
data$IQI <- 0.64
data$IQI[c(2, 5, 8, 11)] <- c(0.63, 0.63, 0.63, 0.63)

data <- consecutive_stations(data)
knitr::kable(data$sample_point_checks[1, c("twoConsecutiveStations")],
  col.names = "Adjacent"
)
```

```{r non_adajcent_plot, results='hide', fig.keep='all'}
probs <- probability_non_linear(data$survey_data, loess = TRUE)
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)

# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  # '#fdae61',
  "#abdda4",
  "#fecc5c"
  # '#2b83ba'
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  # "Poor",
  "Good",
  "Moderate"
  # "High"
)
names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

### ❌ 3rd station between 150 and 200 m required on primary transect

On primary transects, to confirm deposition is not accumulating further from the farm, results for a 3rd station between 150 and 200 m from the pens must be provided if the 2nd of the adjacent stations at Good is less than 100 m from the pens.

*Note* - do we have a way to know the primary transects? Are they labelled in the survey data? Maybe they are always transects 1 and 3??

```{r}
# Create mock survey dataframe
data <- demo_iqi
data <- data[c(1, 2, 3, 10:30), ]
# data$IQI <- 0.64
data$IQI[1:3] <- 0.64
data <- consecutive_stations(data)
knitr::kable(data$sample_point_checks[1, c("twoConsecutiveStations")],
  col.names = "Adjacent"
)
knitr::kable(data$survey_data[data$survey_data$Transect == 1, c("Transect", "Station", "Distance")])
```

**However this rule is not yet applied in the model/app?**. Note all stations on the transect 1 are Good status but 3rd station is not 150-300 even though 2nd station is less than 100m.

```{r, results='hide', fig.keep='all'}
probs <- probability_non_linear(data$survey_data, loess = TRUE)
overrides <- override(probs)
breachs <- breach(overrides)

knitr::kable(data$survey_data[data$survey_data$Transect == 1, c("Transect", "Distance")])

knitr::kable(breachs$breachPositionBestFit[, c(3, 4)])
knitr::kable(breachs$breachPositionBestFit)
```

```{r, results='hide', fig.keep='all'}

areas <- area(breachs)

# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  "#fdae61",
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)
names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

### ❌ Model does not provide probability of pass/fail

There are situation where results remain just above 0.64 along multiple transects. Although the area passes, there is a probablilty that a small decrease in IQI would make the farm fail. This confidence in the result is not always clear and a farm may be well within their footprint but still be relatively close to non-compliance.

Providing Compliance staff and operators with confidence estimates helps guide a course of action when cases are borderline. Currently, the model doesn't assess confidence, if the model predicts the transect doesn't reach good status (on any of the 500 bootstraps), it defaults to the reduced sampling rules. This results in lacking a numerical understanding of confidence and also means applying more complicated rules.

We propose returning all the bootstrapped predictions including predictions which are less than compliant. The percentage of predictions reaching good (from the 500 bootstraps) will be the confidence/probability of pass/fail.

### ❌ Model is slow and complicated

The current model code is 1000 lines. This is a lot for what is a fairly easy model (IQI Vs Distance). Including all the code this is 2000 lines. We expect problems like this to be solved within 100-200 lines. This indicates to us that we have gone off the normal statistical beaten track.

The modelling is also slow, which is annoying in practice but also indicates that what is a simple processes has been over-complicated.

The current process takes each transect separately, and applies several models to IQI and Distance variables to find the best fitting. It is trying to predict the IQI outcome based on the Distance predictor. These variables are independent and have no casual link. The model fitting isn't trying to understand the physical processes and we have no theoretical framework to know if a quadratic or polynomial model should explain the patterns we see. Furthermore, we are not planning to re-use the model on other data, we are creating a new model for each transect. We have no intention to create a generalised model explaining how IQI improves the further from a pollution source we sample or explain the variables which may predict this response curve.

In this situation, it is then apt to use a smooth line model. Just a shifting localised mean. In Excel this is like adding a 'smooth line'. More formally it's a Loess model or local fitting model.

Changing just this approach speeds up the model 5-10x. Making the model more understandable and practical (faster) to use.

Suggest using Loess version:

```{r, results='hide'}

# Create mock survey dataframe
data <- demo_iqi
data <- consecutive_stations(demo_iqi)
start <- Sys.time()
```

```{r, results='hide'}
probs <- probability_non_linear(data$survey_data, loess = TRUE)
```

```{r}
end <- Sys.time()
end - start
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)
```

```{r}
# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  "#fdae61",
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)
names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

Standard version:

```{r}
# Create mock survey dataframe
data <- consecutive_stations(demo_iqi)
start <- Sys.time()
```

```{r, results='hide'}
probs <- probability_non_linear(data$survey_data)
```

```{r}
end <- Sys.time()
end - start
overrides <- override(probs)
breachs <- breach(overrides)
areas <- area(breachs)
```

```{r}
# Convert survey data to spatial
data <- st_as_sf(data$survey_data, coords = c("Longitude", "Latitude"), crs = 4326)
# Calculate area without overrides
ellipse <- areas$ellipse
myColors <- c(
  # '#d7191c',
  "#fdae61",
  "#fecc5c",
  "#abdda4",
  "#2b83ba"
)
data$`WFD status` <- as.factor(data$`WFD status`)
data$`WFD status` <- fct_relevel(
  data$`WFD status`,
  # "Bad",
  "Poor",
  "Moderate",
  "Good",
  "High"
)
names(myColors) <- levels(data$`WFD status`)
colScale <- scale_colour_manual(name = "WFD status", values = myColors)
g <- ggplot() +
  geom_sf(data = data, aes(color = `WFD status`)) +
  geom_sf(data = ellipse, alpha = 0) +
  colScale +
  blue_theme
g
```

### ✅ Or alternative?

Calibrate the newdeepmod output? Here's the predicted footprint with Good status outlined in black.

```{r}
library(raster)
volcano <- datasets::volcano
orig <- volcano

volcano[volcano < 140] <- NA
volcano <- raster(volcano)

r <- volcano > -Inf
# or alternatively
# r <- reclassify(x, cbind(-Inf, Inf, 1))

# convert to polygons (you need to have package 'rgeos' installed for this to work)
pp <- rasterToPolygons(r, dissolve = TRUE)
plot(raster(orig))
plot(pp, add = TRUE)
```

We then recalibrate this modelled 3D surface of modelled IQI scores with observed IQI. Taking the average difference between predicted IQI and observed IQI. (Or more sophisticate interpolation along transects 'poles' and modelled surface?).

```{r}

volcano <- datasets::volcano
orig <- volcano

plot(raster(orig))
orig[orig < 130] <- NA

orig <- raster(orig)
r <- orig > -Inf

# convert to polygons (you need to have package 'rgeos' installed for this to work)
pp <- rasterToPolygons(r, dissolve = TRUE)
plot(pp, add = TRUE)
```

This is a bit like weather prediction, we have some understanding of the fundamental physical processes, make a prediction based on them and then update with new observations.

This avoids assumption of ellipse shape and gracefully deals with missing data (transects). Puts us on a bases to further improve and build understanding and better models in future.


